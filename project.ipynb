{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "import seaborn as sns\n",
    "from weighted_spearman import weighted_spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Respiratory track conditions have statistically significant positive correlation with air pollutants.\n",
    "- Overall quality of life metrics have statistically significant negavite correlation with air pollutants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and cleaning the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambient air quality data from World Health Organisation (WHO). The dataset has measurements for pm25, pm10 and NO2 levels for 7000 human settlements in 120 countries. We restrict the scope of this analysis to only include cities in the USA, since the most uniform health data for a large area we found was for the USA.\n",
    "\n",
    "[link](https://www.who.int/publications/m/item/who-ambient-air-quality-database-(update-jan-2024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the air quality data\n",
    "air_quality_df = pd.read_excel(\"who_ambient_air_quality_database_version_2024_(v6.1).xlsx\", sheet_name=\"Update 2024 (V6.1)\")\n",
    "\n",
    "# Filter by US\n",
    "air_quality_df = air_quality_df[air_quality_df[\"country_name\"] == \"United States of America\"]\n",
    "\n",
    "# Drop unnnecessary columns\n",
    "air_quality_df = air_quality_df.drop(['country_name', 'version','reference','web_link', 'population_source','who_ms', 'type_of_stations', 'population', 'latitude', 'longitude', 'iso3', 'who_region', 'pm25_tempcov', 'pm10_tempcov','no2_tempcov'], axis=1)\n",
    "\n",
    "# Remove state from city names\n",
    "air_quality_df['city'] = air_quality_df['city'].str.split(' ').str[0]\n",
    "air_quality_df['city'] = air_quality_df['city'].str.split('-').str[0]\n",
    "\n",
    "# Leave only year 2020\n",
    "air_quality_df = air_quality_df[air_quality_df['year'] == 2020.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics of different medical conditions in USA cities. The dataset includes different health measures, risk behaviours and preventive measures taken in different US cities. In the context of this analysis we are only interested in the health measures, so we remove all the irrelevant columns. However, we include smoking among adults for a reason we will discuss during the analysis of the results.\n",
    "\n",
    "[link](https://data.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-County-Data-20/duw2-7jbt/about_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the health statistics data\n",
    "health_statistic_df = pd.read_csv(\"PLACES_Local_Data_for_Better_Health_County_Data_2022_release_20250320.csv\")\n",
    "\n",
    "# We will use the age-adjusted data for the analysis to mitigate the impact of age distribution differences across cities\n",
    "health_statistic_df = health_statistic_df[health_statistic_df[\"Data_Value_Type\"].str.contains(\"Age\", na=False)]\n",
    "\n",
    "# Filter out cities with less than 100,000 population\n",
    "health_statistic_df = health_statistic_df[health_statistic_df['TotalPopulation'] > 100000]\n",
    "\n",
    "# Rename the city column\n",
    "health_statistic_df = health_statistic_df.rename(columns={\"LocationName\": \"city\"})\n",
    "\n",
    "# Drop unnecessary columns\n",
    "health_statistic_df = health_statistic_df.drop(['StateAbbr', 'StateDesc', 'DataSource', 'Category', 'LocationID', 'CategoryID', 'DataValueTypeID', 'Geolocation', 'Low_Confidence_Limit', 'High_Confidence_Limit', 'Data_Value_Footnote_Symbol', 'MeasureId', 'Data_Value_Footnote' ], axis=1)\n",
    "\n",
    "# First, let's identify the unique cities and measures\n",
    "unique_cities = health_statistic_df['city'].unique()\n",
    "unique_measures = health_statistic_df['Short_Question_Text'].unique()\n",
    "\n",
    "# Create a pivot table with cities as index and measures as columns\n",
    "health_df = health_statistic_df.pivot_table(\n",
    "    index='city',\n",
    "    columns='Measure',\n",
    "    values='Data_Value',\n",
    "    aggfunc='mean'  # Use mean if there are duplicate entries\n",
    ")\n",
    "\n",
    "population_df = health_statistic_df.groupby('city')['TotalPopulation'].first().reset_index()\n",
    "\n",
    "# Reset the index to make 'city' a column again\n",
    "health_df = health_df.reset_index()\n",
    "\n",
    "# Drop columns that are not needed for the analysis\n",
    "conditionsToDrop = [\n",
    "'Current lack of health insurance among adults aged 18-64 years',\n",
    "'Cervical cancer screening among adult women aged 21-65 years',\n",
    "'Visits to dentist or dental clinic among adults aged >=18 years',\n",
    "'Visits to doctor for routine checkup within the past year among adults aged >=18 years',\n",
    "'Mammography use among women aged 50-74 years',\n",
    "'Cholesterol screening among adults aged >=18 years',\n",
    "'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',\n",
    "'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',\n",
    "'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',\n",
    "'All teeth lost among adults aged >=65 years',\n",
    "'Arthritis among adults aged >=18 years',\n",
    "'Binge drinking among adults aged >=18 years',\n",
    "# 'Current smoking among adults aged >=18 years',\n",
    "'Diagnosed diabetes among adults aged >=18 years',\n",
    "'High cholesterol among adults aged >=18 years who have been screened in the past 5 years',\n",
    "'No leisure-time physical activity among adults aged >=18 years',\n",
    "'No leisure-time physical activity among adults aged >=18 years',\n",
    "'Sleeping less than 7 hours among adults aged >=18 years',\n",
    "'Stroke among adults aged >=18 years',\n",
    "'Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure'\n",
    "]\n",
    "health_df = health_df.drop(conditionsToDrop, axis=1)\n",
    "\n",
    "# All diseases\n",
    "numberOfDiseases = len(list(health_statistic_df['Measure'].unique()))\n",
    "\n",
    "\n",
    "health_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching cities using rapidfuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the formatting of some city names is slightly different in the two datasets, using fuzzywuzzy allows us to keep more of the data than simple string matching. In this block we replace the city names in 'city' column of air_quality_df with the corresponding names from health_df so that we can merge them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match_city(city, citylist, threshold=95):\n",
    "    best_match, score, _ = process.extractOne(city, citylist, scorer=fuzz.ratio)\n",
    "    return best_match if score >= threshold else city\n",
    "\n",
    "air_quality_df['city'] = air_quality_df['city'].apply(lambda x: find_best_match_city(x, unique_cities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the two based on the matched cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the previously handled 'city' column to merge our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes on the 'city' column\n",
    "# using inner join to keep only cities that are present in both dataframes\n",
    "merged_df = pd.merge(health_df, air_quality_df, on='city', how='inner')\n",
    "\n",
    "# Merge with population data\n",
    "# using left join to keep all cities from merged_df\n",
    "merged_df = pd.merge(merged_df, population_df, on='city', how='left')\n",
    "\n",
    "# Drop duplicate cities, because the data contains multiple entries for the same city\n",
    "merged_df = merged_df.drop_duplicates(subset='city', keep='first')\n",
    "\n",
    "print(\"Columns with non-null values for:\")\n",
    "print(f\"PM10 concentrations  - {merged_df['pm10_concentration'].notna().sum()}\")\n",
    "print(f\"PM2.5 concentrations - {merged_df['pm25_concentration'].notna().sum()}\")\n",
    "print(f\"NO2 concentrations   - {merged_df['no2_concentration'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the normality of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can look for correlations in the data, we need to test for the normality of the data to be able to use appropriate formula for the correlation. Hence, we perform the Shapiro-Wilk normality test, because it is best suited for datasets with medium to large sample sizes. After running the test, we conclude that some of the diseases appear to be non-normally distributed, therefore we must use a formula for correlation coefficient that is suitable for non-normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = merged_df.to_numpy()\n",
    "\n",
    "p_values = []\n",
    "\n",
    "# Get the column names for disease columns (skipping city, year, pollutants, and population)\n",
    "disease_columns = merged_df.columns[1:13].tolist()\n",
    "print(f\"Testing normality for {len(disease_columns)} disease columns:\")\n",
    "print(\"-\"*110)\n",
    "\n",
    "# Run Shapiro-Wilk test for each disease column\n",
    "for i, col_name in enumerate(disease_columns):\n",
    "    disease_data = merged_df[col_name].dropna().values\n",
    "    \n",
    "    stat, p_value = shapiro(disease_data)\n",
    "    p_values.append(p_value)\n",
    "    \n",
    "    result = \"potentially normal\" if p_value >= 0.05 else \"non-normal\"\n",
    "    \n",
    "    print(f\"{col_name:70} | {result:20} | p={p_value:.3f}\")\n",
    "\n",
    "\n",
    "# Count how many p-values are less than 0.05 (indicating non-normal distribution)\n",
    "non_normal_count = sum(p < 0.05 for p in p_values)\n",
    "print(\"-\"*110)\n",
    "col_name = \"Potentially normal\"\n",
    "print(f\"{col_name:70} | {len(p_values) - non_normal_count}\")\n",
    "col_name = \"Non-normal\"\n",
    "print(f\"{col_name:70} | {non_normal_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted spearman's correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use spearman's correlation, because it's well suited for non-normally distributed data. Additionally we use the populations of the cities as weights to take into account differently sized cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the correlation and p-value matrixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the correlation matrix which includes the correlation coefficient for each pollutant-disease pair. We also create a corresponding p-value matrix, since calculating the p-value for weighted spearman's correlation afterwards is hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pollutant columns, we choose to exlude NO2 here, since there was less data on that one then the other\n",
    "# pollutants, and the relaibility of the data appeared to be worse, since the measurements were less frequent.\n",
    "pollutants = ['pm10_concentration', 'pm25_concentration']\n",
    "\n",
    "# Disease columns\n",
    "disease_columns = [col for col in health_df.columns if col != 'city']\n",
    "\n",
    "# Create a correlation matrix between pollutants and diseases\n",
    "correlation_matrix = pd.DataFrame(index=disease_columns, columns=pollutants)\n",
    "\n",
    "# Create corresponding p-value matrix for the correlations\n",
    "p_value_matrix = pd.DataFrame(index=disease_columns, columns=pollutants)\n",
    "\n",
    "# Extract weights (population values)\n",
    "weights = merged_df['TotalPopulation']\n",
    "\n",
    "# Calculate weighted correlation for each pollutant-disease pair\n",
    "for disease in disease_columns:\n",
    "    for pollutant in pollutants:\n",
    "        # Extract the data\n",
    "        data_i = merged_df[disease]\n",
    "        data_j = merged_df[pollutant]\n",
    "        \n",
    "        # Create mask for non-NaN values in both columns\n",
    "        mask = ~data_i.isna() & ~data_j.isna()\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if mask.sum() < 3:\n",
    "            correlation_matrix.loc[disease, pollutant] = np.nan\n",
    "            p_value_matrix.loc[disease, pollutant] = np.nan\n",
    "            continue\n",
    "        \n",
    "        # Calculate weighted correlation using the function\n",
    "        try:\n",
    "            corr_value, p_value = weighted_spearman(\n",
    "                data_i[mask].values, \n",
    "                data_j[mask].values, \n",
    "                weights[mask].values\n",
    "            )\n",
    "            correlation_matrix.loc[disease, pollutant] = corr_value\n",
    "            p_value_matrix.loc[disease, pollutant] = p_value\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating correlation for {disease} and {pollutant}: {e}\")\n",
    "            correlation_matrix.loc[disease, pollutant] = np.nan\n",
    "            p_value_matrix.loc[disease, pollutant] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying correlation matrix without p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the correlation matrix between airpollutants and medical conditions, since it is the easiest way to get a comprehensive view of all the correlations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and transpose the correlation matrix - diseases as rows and pollutants as columns\n",
    "disease_pollutant_corr = correlation_matrix.loc[disease_columns, pollutants]\n",
    "\n",
    "# Visualize the correlation matrix as a heatmap using matplotlib\n",
    "plt.figure(figsize=(10, 14))\n",
    "\n",
    "# Convert correlation values to float\n",
    "disease_pollutant_corr = disease_pollutant_corr.astype(float)\n",
    "\n",
    "# Create the heatmap\n",
    "im = plt.imshow(disease_pollutant_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Correlation Coefficient')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Correlation between Air Pollutants and Medical Conditions', pad=50)\n",
    "plt.xticks(np.arange(len(pollutants)), pollutants, rotation=-35, ha='left')\n",
    "plt.yticks(np.arange(len(disease_columns)), disease_columns, rotation=0)  # Changed rotation to 0\n",
    "\n",
    "# Add correlation values as text annotations\n",
    "for i in range(len(disease_columns)):\n",
    "    for j in range(len(pollutants)):\n",
    "        corr_value = disease_pollutant_corr.iloc[i, j]\n",
    "        \n",
    "        # Format the correlation value\n",
    "        if pd.notna(corr_value):\n",
    "            corr_text = f'{corr_value:.2f}'\n",
    "            plt.text(j, i, corr_text, ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('corr_weighted_spearman.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating correlation matrix with p-values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the correlation matrix with the corresponding p-values to better understand the results. Correlations that are not statistically significant with 5% significanse level are grayed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 14))\n",
    "\n",
    "# Create the heatmap\n",
    "im = plt.imshow(disease_pollutant_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Correlation Coefficient')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Correlation between Air Pollutants Health Measures With p-values', pad=50)\n",
    "plt.xticks(np.arange(len(pollutants)), pollutants, rotation=-35, ha='left')\n",
    "plt.yticks(np.arange(len(disease_columns)), disease_columns, rotation=0)\n",
    "\n",
    "ax = plt.gca()  # Get the current axis\n",
    "\n",
    "# Add correlation values, p-values, and light gray overlay\n",
    "for i in range(len(disease_columns)):\n",
    "    for j in range(len(pollutants)):\n",
    "        corr_value = disease_pollutant_corr.iloc[i, j]\n",
    "        p_value = p_value_matrix.iloc[i, j]\n",
    "        \n",
    "        # Format the correlation value\n",
    "        corr_text = f'{corr_value:.2f}'\n",
    "        \n",
    "        # Format the p-value\n",
    "        if pd.notna(p_value):\n",
    "            if p_value < 0.001:\n",
    "                p_text = 'p<0.001'\n",
    "            elif p_value < 0.01:\n",
    "                p_text = 'p<0.01'\n",
    "            elif p_value < 0.05:\n",
    "                p_text = 'p<0.05'\n",
    "            else:\n",
    "                p_text = f'p={p_value:.2f}'\n",
    "        else:\n",
    "            p_text = 'p=NA'\n",
    "\n",
    "        # Add text annotations\n",
    "        plt.text(j, i - 0.15, corr_text, ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "        plt.text(j, i + 0.15, p_text, ha=\"center\", va=\"center\", color=\"black\", fontsize=7)\n",
    "\n",
    "        # Add LIGHT gray overlay if p-value >= 0.05\n",
    "        if p_value >= 0.05:\n",
    "            rect = plt.Rectangle((j - 0.5, i - 0.5), 1, 1, color='#B0B0B0')  # Light gray\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('corr_weighted_spearman_lightgray.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings from the data\n",
    "\n",
    "- 'Physical health not good for >= 14 days amon adults' and 'Fair or poor self-rated health status among adults' seem to positively correlate with both pm10 and pm25 pollution levels, meaning that the overall health of individuals seems to negatively correlate with air pollution.\n",
    "\n",
    "- Prevalence of neither asthma nor chronic obstructive pulmonary disease have significant correlation with the pollution levels, even though both are respiratory track conditions.\n",
    "\n",
    "- Prevalence of cancer among adults seems to negatively correlate with pollution levels, which seems counter intuitive. For this reason, we wanted to include current smoking among adults in the results, since at least in within the scope of our data it seemed to negatively correlate with air pollution levels, and as a known predictor for cancer it could partially explain the result. Hence it could be one hidden variable contributing to this correlation, but the scope of this analysis isn't wide enough to make any conclusions for that. Here it is important to remember that correlation and causation are distinct.\n",
    "\n",
    "- Prevelance of chronic kidney disease appears to have positive correlation with air pollutants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating scatter plots of cities for the air pollutant-medical condition pairs that had statistically significant correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's handle the filtering of correlations without modifying disease_columns\n",
    "insignificant_mask = p_value_matrix > 0.05\n",
    "ins_filtered_correlations = correlation_matrix.copy()\n",
    "ins_filtered_correlations = ins_filtered_correlations.where(insignificant_mask, np.nan)\n",
    "ins_filtered_correlations = ins_filtered_correlations.astype(float)  # Ensure numeric dtype\n",
    "\n",
    "# Get the columns that have insignificant p-values\n",
    "invalid_rows = insignificant_mask.any(axis=1)\n",
    "\n",
    "# Get the disease names corresponding to these rows (assuming correlation_matrix has disease names as index)\n",
    "if len(correlation_matrix.index) == len(disease_columns):\n",
    "    columns_with_insignificant_values = [disease_columns[i] for i in range(len(disease_columns)) if invalid_rows[i]]\n",
    "else:\n",
    "    # If the indices don't match directly, we need another approach\n",
    "    # This is a safer approach if the structure is unclear\n",
    "    columns_with_insignificant_values = []\n",
    "    print(\"Warning: Correlation matrix index length doesn't match disease_columns length.\")\n",
    "\n",
    "# Make a copy of merged_df\n",
    "xd_df = merged_df.copy()\n",
    "\n",
    "# Drop columns with insignificant values\n",
    "xd_df = xd_df.drop(columns_with_insignificant_values, axis=1, errors='ignore')\n",
    "\n",
    "# Select the diseases with statistically significant correlations with 5% significance level\n",
    "# disease_columns = [\n",
    "#     \"Cancer (excluding skin cancer) among adults aged >=18 years\",\n",
    "#     \"Chronic kidney disease among adults aged >=18 years\",\n",
    "#     \"Fair or poor self-rated health status among adults aged >=18 years\",\n",
    "#     \"Physical health not good for >=14 days among adults aged >=18 years\"\n",
    "# ]\n",
    "\n",
    "pollutant_columns = [\"pm10_concentration\", \"pm25_concentration\"]  # No no2_concentration\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create scatter plots only for diseases that exist in the dataframe\n",
    "# Create a figure with subplots in a 2x2 grid (one for each disease-pollutant combination)\n",
    "fig, axes = plt.subplots(4, 2, figsize=(14, 24))\n",
    "axes = axes.flatten()  # Flatten to make indexing easier\n",
    "\n",
    "# Track which plots we've created\n",
    "plot_count = 0\n",
    "\n",
    "# Create scatter plots for the specified diseases\n",
    "for i, disease in enumerate(disease_columns):\n",
    "    if disease in xd_df.columns:\n",
    "        for j, pollutant in enumerate(pollutant_columns):\n",
    "            # Create the scatter plot in the appropriate subplot position\n",
    "            ax = axes[plot_count]\n",
    "            \n",
    "            # Create scatter plot with population-sized markers\n",
    "            scatter = sns.scatterplot(\n",
    "                data=xd_df, \n",
    "                x=pollutant, \n",
    "                y=disease, \n",
    "                size=\"TotalPopulation\",  # Size points by population\n",
    "                sizes=(20, 200),        # Set range of point sizes\n",
    "                alpha=0.7,              # Add transparency\n",
    "                ax=ax\n",
    "            )\n",
    "            \n",
    "            # Add a trend line\n",
    "            sns.regplot(\n",
    "                data=xd_df,\n",
    "                x=pollutant, \n",
    "                y=disease,\n",
    "                scatter=False,\n",
    "                ci=95,\n",
    "                line_kws={'color': 'red'},\n",
    "                ax=ax\n",
    "            )\n",
    "            \n",
    "            ax.set_xlabel(f\"{pollutant}\", fontsize=10)\n",
    "            ax.set_ylabel('Age adjusted prevalence (%)', fontsize=10)\n",
    "            ax.set_title(f\"{disease}\\nvs {pollutant}\", fontsize=12)\n",
    "            plot_count += 1\n",
    "    else:\n",
    "        print(f\"{disease} was filtered out due to insignificant correlations\")\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample scatter plots for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Use the original merged_df, and include all disease columns\n",
    "all_disease_columns = [col for col in merged_df.columns if \"among adults aged\" in col]\n",
    "pollutant_columns = [\"pm10_concentration\", \"pm25_concentration\"]\n",
    "\n",
    "# Create a 3x4 grid of subplots with specified figure size\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 10), constrained_layout=True)\n",
    "axes = axes.flatten()  # Flatten to make indexing easier\n",
    "\n",
    "# Keep track of the current subplot\n",
    "plot_index = 0\n",
    "\n",
    "# Iterate through diseases and pollutants to create scatter plots\n",
    "for disease in all_disease_columns:\n",
    "    for pollutant in pollutant_columns:\n",
    "        # Skip if we've run out of subplots\n",
    "        if plot_index >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[plot_index]  # Select the current subplot\n",
    "        plot_index += 1\n",
    "        \n",
    "        # Create scatter plot in the current subplot (no regression line)\n",
    "        sns.scatterplot(data=merged_df, x=pollutant, y=disease, ax=ax)\n",
    "        \n",
    "        # Set labels and titles\n",
    "        ax.set_xlabel(pollutant)\n",
    "        ax.set_ylabel('Age-adjusted prevalence')\n",
    "        \n",
    "        # Create a shorter title\n",
    "        disease_short = disease.replace(\" among adults aged >=18 years\", \"\")\n",
    "        pollutant_short = pollutant.replace(\"_concentration\", \"\")\n",
    "        ax.set_title(f\"{disease_short} vs {pollutant_short}\", fontsize=10)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(plot_index, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "# Add a main title\n",
    "fig.suptitle('Disease Prevalence vs Air Pollution', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating p-values for the correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole_population = np.sum(array[:,numberOfDiseases-1])\n",
    "# print(whole_population)\n",
    "# wheighting_array = array[:,numberOfDiseases]/whole_population\n",
    "# wheighting_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This is a test correlation for one disease \n",
    "\n",
    "# X1=array[:,14]*wheighting_array\n",
    "# Y=array[:,23]\n",
    "# print(array[:,14])\n",
    "# print(X1)\n",
    "\n",
    "# x = X1.astype(float)\n",
    "# y = Y.astype(float)\n",
    "\n",
    "# mask = ~np.isnan(y)& ~np.isnan(x)\n",
    "\n",
    "# Y_filtered=y[mask]\n",
    "# X1_filtered=x[mask]\n",
    "\n",
    "# res = stats.pearsonr(X1_filtered, Y_filtered)\n",
    "\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find out whether the data is normally distributed so that we know how to compute the correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find this result extremely suspicious hence we use different test satatistics for checking normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find that the data is indeed normally distributed and as KS test is very prone to overinterpretation of outlying values and not suited for large datasets, we trust Shapiro-Wilk test and decide to go for pearson correlation test without permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = array.shape[1]\n",
    "\n",
    "air_pollution_parameter = [n-3,n-2,n-1]\n",
    "disease_range_start = 1\n",
    "disease_range_end = numberOfDiseases\n",
    "\n",
    "#permutation_model=stats.PermutationMethod(n_resamples=99999, batch=None, random_state=None, rng=None)\n",
    "\n",
    "# Ensure Corr array matches the range\n",
    "Corr_p_values = np.zeros((disease_range_end - disease_range_start + 1,len(air_pollution_parameter)))\n",
    "Corr_coef = np.zeros((disease_range_end - disease_range_start + 1,len(air_pollution_parameter)))\n",
    "\n",
    "#This computes p-values of pearson correlation test statistic between diseases and different air pollutants and puts it into a 21x3 array\n",
    "for j in range(0,len(air_pollution_parameter)):\n",
    "    y = array[:, air_pollution_parameter[j]]\n",
    "\n",
    "    for i in range(disease_range_start, disease_range_end + 1):\n",
    "        try:\n",
    "            # Extract disease column\n",
    "            a = array[:, i]#*wheighting_array\n",
    "            \n",
    "            # Convert to float as datasets are super messy\n",
    "            a_float = a.astype(float)\n",
    "            y_float = y.astype(float)\n",
    "            \n",
    "            # Create mask for non-NaN values in BOTH columns\n",
    "            mask =  ~np.isnan(y_float) & ~np.isnan(a_float) \n",
    "            \n",
    "            # Filter both arrays\n",
    "            y_filtered = y_float[mask]\n",
    "            a_filtered = a_float[mask]\n",
    "            \n",
    "            # Compute Pearson correlation\n",
    "            res = stats.pearsonr(a_filtered, y_filtered)\n",
    "            \n",
    "            Corr_p_values[i - disease_range_start,j] = res.pvalue\n",
    "            Corr_coef[i - disease_range_start,j]=res.correlation\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column {i}: {e}\")\n",
    "\n",
    "print(Corr_coef)\n",
    "Corr_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a scatter plot of diseases and thier p-values\n",
    "\n",
    "features = np.arange(1, len(Corr_p_values) + 1)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(features, Corr_p_values[:,0], color='blue', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_p_values[:,1], color='green', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_p_values[:,2], color='red', edgecolors='black', alpha=0.7)\n",
    "# Red line at y=0.05\n",
    "plt.axhline(y=0.05, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.xlim(1, len(Corr_p_values))\n",
    "\n",
    "plt.xticks(features)\n",
    "\n",
    "plt.xlabel('Diseases')\n",
    "plt.ylabel('correlation p-value')\n",
    "plt.title('Correlation Scatter Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify lengths\n",
    "print(\"Length of Corr:\", len(Corr_p_values))\n",
    "print(\"Length of features:\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This filters the Corr matrix so that only statistically significant values (p<=0.05) remain\n",
    "print(Corr_p_values.shape)\n",
    "\n",
    "Corr_significat_p=Corr_p_values.copy() #if I dont use copy(), Corr also changes when I change Corr_significant\n",
    "Corr_significat_p[Corr_significat_p > 0.05] = np.nan\n",
    "print(Corr_significat_p.shape)\n",
    "print(Corr_significat_p)\n",
    "\n",
    "#This creates a scatter plot with statistically significant values\n",
    "\n",
    "features = np.arange(1, len(Corr_significat_p) + 1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(features, Corr_significat_p[:,0], color='blue', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_significat_p[:,1], color='green', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_significat_p[:,2], color='red', edgecolors='black', alpha=0.7)\n",
    "# Red line at y=0.05\n",
    "plt.axhline(y=0.05, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.xlim(1, len(Corr_p_values))\n",
    "\n",
    "plt.xticks(features)\n",
    "\n",
    "plt.xlabel('Diseases')\n",
    "plt.ylabel('correlation p-value')\n",
    "plt.title('Correlation Scatter Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify lengths\n",
    "print(\"Length of Corr_significant:\", len(Corr_significat_p))\n",
    "print(\"Length of features:\", len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as I want to present the data as a table I switch to dataframes as it makes it easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_p_value_df= pd.DataFrame(Corr_p_values, index=disease_columns, columns=pollutants)\n",
    "\n",
    "significant_correlation_p_value_df = correlation_p_value_df.where(correlation_p_value_df <= 0.05, np.nan)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=correlation_p_value_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=correlation_p_value_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=significant_correlation_p_value_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=significant_correlation_p_value_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to filter the correlation coefficient data so that it leaves only the statistically signifficant coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_Corr_coef = np.where(Corr_p_values < 0.05, Corr_coef, np.nan)\n",
    "sign_Corr_coef\n",
    "\n",
    "correlation_coef_df= pd.DataFrame(Corr_coef, index=disease_columns, columns=pollutants)\n",
    "sign_correlation_coef_df= pd.DataFrame(sign_Corr_coef, index=disease_columns, columns=pollutants)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=correlation_coef_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=correlation_coef_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=sign_correlation_coef_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=sign_correlation_coef_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
