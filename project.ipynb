{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "import seaborn as sns\n",
    "from weighted_spearman import weighted_spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and cleaning the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambient air quality data from World Health Organisation (WHO)\n",
    "\n",
    "[link](https://www.who.int/publications/m/item/who-ambient-air-quality-database-(update-jan-2024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_df = pd.read_excel(\"who_ambient_air_quality_database_version_2024_(v6.1).xlsx\", sheet_name=\"Update 2024 (V6.1)\")\n",
    "\n",
    "\n",
    "# Filter by US\n",
    "air_quality_df = air_quality_df[air_quality_df[\"country_name\"] == \"United States of America\"]\n",
    "\n",
    "# Drop unnnecessary columns\n",
    "air_quality_df = air_quality_df.drop(['country_name', 'version','reference','web_link', 'population_source','who_ms', 'type_of_stations', 'population', 'latitude', 'longitude', 'iso3', 'who_region', 'pm25_tempcov', 'pm10_tempcov','no2_tempcov'], axis=1)\n",
    "\n",
    "# Remove state from city names\n",
    "air_quality_df['city'] = air_quality_df['city'].str.split(' ').str[0]\n",
    "air_quality_df['city'] = air_quality_df['city'].str.split('-').str[0]\n",
    "# Leave only year 2020\n",
    "air_quality_df = air_quality_df[air_quality_df['year'] == 2020.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics of different medical conditions in USA cities.\n",
    "\n",
    "[link](https://data.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-County-Data-20/duw2-7jbt/about_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_statistic_df = pd.read_csv(\"PLACES_Local_Data_for_Better_Health_County_Data_2022_release_20250320.csv\")\n",
    "\n",
    "health_statistic_df = health_statistic_df[health_statistic_df[\"Data_Value_Type\"].str.contains(\"Age\", na=False)]\n",
    "health_statistic_df = health_statistic_df[health_statistic_df['TotalPopulation'] > 100000]\n",
    "\n",
    "# Rename the city column\n",
    "health_statistic_df = health_statistic_df.rename(columns={\"LocationName\": \"city\"})\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "health_statistic_df = health_statistic_df.drop(['StateAbbr', 'StateDesc', 'DataSource', 'Category', 'LocationID', 'CategoryID', 'DataValueTypeID', 'Geolocation', 'Low_Confidence_Limit', 'High_Confidence_Limit', 'Data_Value_Footnote_Symbol', 'MeasureId', 'Data_Value_Footnote' ], axis=1)\n",
    "# First, let's identify the unique cities and measures\n",
    "unique_cities = health_statistic_df['city'].unique()\n",
    "unique_measures = health_statistic_df['Short_Question_Text'].unique()\n",
    "\n",
    "# print(f\"Number of unique cities: {len(unique_cities)}\")\n",
    "# print(f\"Number of unique measures: {len(unique_measures)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a pivot table with cities as index and measures as columns\n",
    "health_df = health_statistic_df.pivot_table(\n",
    "    index='city',\n",
    "    columns='Measure',\n",
    "    values='Data_Value',\n",
    "    aggfunc='mean'  # Use mean if there are duplicate entries\n",
    ")\n",
    "\n",
    "population_df = df.groupby('city')['TotalPopulation'].first().reset_index()\n",
    "\n",
    "# Reset the index to make 'city' a column again\n",
    "health_df = health_df.reset_index()\n",
    "\n",
    "# Drop columns that are not needed for the analysis\n",
    "conditionsToDrop = [\n",
    "'Current lack of health insurance among adults aged 18-64 years',\n",
    "'Cervical cancer screening among adult women aged 21-65 years',\n",
    "'Visits to dentist or dental clinic among adults aged >=18 years',\n",
    "'Visits to doctor for routine checkup within the past year among adults aged >=18 years',\n",
    "'Mammography use among women aged 50-74 years',\n",
    "'Cholesterol screening among adults aged >=18 years',\n",
    "'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',\n",
    "'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',\n",
    "'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',\n",
    "'All teeth lost among adults aged >=65 years',\n",
    "'Arthritis among adults aged >=18 years',\n",
    "'Binge drinking among adults aged >=18 years',\n",
    "# 'Current smoking among adults aged >=18 years',\n",
    "'Diagnosed diabetes among adults aged >=18 years',\n",
    "'High cholesterol among adults aged >=18 years who have been screened in the past 5 years',\n",
    "'No leisure-time physical activity among adults aged >=18 years',\n",
    "'No leisure-time physical activity among adults aged >=18 years',\n",
    "'Sleeping less than 7 hours among adults aged >=18 years',\n",
    "'Stroke among adults aged >=18 years',\n",
    "'Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure'\n",
    "]\n",
    "health_df = health_df.drop(conditionsToDrop, axis=1)\n",
    "\n",
    "# All diseases\n",
    "numberOfDiseases = len(list(health_statistic_df['Measure'].unique()))\n",
    "print(numberOfDiseases)\n",
    "\n",
    "\n",
    "health_df.head()\n",
    "# print(new_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching cities using fuzzywuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the formatting of some city names is slightly different in the two datasets, using fuzzywuzzy allows us to keep more of the data than simple string matching. In this block we replace the city names in 'city' column of air_quality_df with the corresponding names from health_df so that we can merge them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "def find_best_match_city(city, citylist, threshold=95):\n",
    "    best_match, score, _ = process.extractOne(city, citylist, scorer=fuzz.ratio)\n",
    "    return best_match if score >= threshold else city\n",
    "\n",
    "air_quality_df['city'] = air_quality_df['city'].apply(lambda x: find_best_match_city(x, unique_cities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the two based on the matched cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the previously handled 'city' column to merge our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(health_df, air_quality_df, on='city', how='inner')\n",
    "merged_df = pd.merge(merged_df, population_df, on='city', how='left')\n",
    "merged_df = merged_df.drop_duplicates(subset='city', keep='first')\n",
    "\n",
    "# Optional: print some information about the merged DataFrame\n",
    "# print(f\"Shape of dataframe: {merged_df.shape}\")\n",
    "print(\"Columns with non-null values for:\")\n",
    "print(f\"PM10 concentrations  - {merged_df['pm10_concentration'].notna().sum()}\")\n",
    "print(f\"PM2.5 concentrations - {merged_df['pm25_concentration'].notna().sum()}\")\n",
    "print(f\"NO2 concentrations   - {merged_df['no2_concentration'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the normality of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can look for correlations in the data, we need to test for the normality of the data to be able to use appropriate formula for the correlation. Hence, we perform the Shapiro-Wilk normality test, because it is best suited for datasets with medium to large sample sizes. After running the test, we can conclude that some of the diseases appear to be non-normally distributed, therefore we must use a formula for correlation coefficient that is suitable for non-normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = merged_df.to_numpy()\n",
    "\n",
    "p_values = []\n",
    "\n",
    "# Get the column names for disease columns (skipping city, year, pollutants, and population)\n",
    "disease_columns = merged_df.columns[1:13].tolist()\n",
    "print(f\"Testing normality for {len(disease_columns)} disease columns:\")\n",
    "print(\"-\"*110)\n",
    "\n",
    "# Run Shapiro-Wilk test for each disease column\n",
    "for i, col_name in enumerate(disease_columns):\n",
    "    disease_data = merged_df[col_name].dropna().values\n",
    "    \n",
    "    stat, p_value = shapiro(disease_data)\n",
    "    p_values.append(p_value)\n",
    "    \n",
    "    result = \"potentially normal\" if p_value >= 0.05 else \"non-normal\"\n",
    "    \n",
    "    print(f\"{col_name:70} | {result:20} | p={p_value:.3f}\")\n",
    "\n",
    "\n",
    "# Count how many p-values are less than 0.05 (indicating non-normal distribution)\n",
    "non_normal_count = sum(p < 0.05 for p in p_values)\n",
    "print(\"-\"*110)\n",
    "col_name = \"Potentially normal\"\n",
    "print(f\"{col_name:70} | {len(p_values) - non_normal_count}\")\n",
    "col_name = \"Non-normal\"\n",
    "print(f\"{col_name:70} | {non_normal_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted spearman's correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use spearman's correlation, because it's well suited for non-normally distributed data. Additionally we use the populations of the cities as weights to take into account differently sized cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the correlation and p-value matrixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the correlation matrix which includes the correlation coefficient for each pollutant-disease pair. We also create a corresponding p-value matrix, since calculating the p-value for weighted spearman's correlation afterwards is hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pollutant columns, we choose to exlude NO2 here, since there was less data on that one then the other\n",
    "# pollutants, and the relaibility of the data appeared to be worse, since the measurements were less frequent.\n",
    "pollutants = ['pm10_concentration', 'pm25_concentration']\n",
    "\n",
    "# Disease columns\n",
    "disease_columns = [col for col in health_df.columns if col != 'city']\n",
    "\n",
    "# Create a correlation matrix between pollutants and diseases\n",
    "correlation_matrix = pd.DataFrame(index=disease_columns, columns=pollutants)\n",
    "p_value_matrix = pd.DataFrame(index=disease_columns, columns=pollutants)\n",
    "\n",
    "# Extract weights (population values)\n",
    "weights = merged_df['TotalPopulation']\n",
    "\n",
    "# Calculate weighted correlation for each pollutant-disease pair\n",
    "for disease in disease_columns:\n",
    "    for pollutant in pollutants:\n",
    "        # Extract the data\n",
    "        data_i = merged_df[disease]\n",
    "        data_j = merged_df[pollutant]\n",
    "        \n",
    "        # Create mask for non-NaN values in both columns\n",
    "        mask = ~data_i.isna() & ~data_j.isna()\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if mask.sum() < 3:\n",
    "            correlation_matrix.loc[disease, pollutant] = np.nan\n",
    "            p_value_matrix.loc[disease, pollutant] = np.nan\n",
    "            continue\n",
    "        \n",
    "        # Calculate weighted correlation using the function\n",
    "        try:\n",
    "            corr_value, p_value = weighted_spearman(\n",
    "                data_i[mask].values, \n",
    "                data_j[mask].values, \n",
    "                weights[mask].values\n",
    "            )\n",
    "            correlation_matrix.loc[disease, pollutant] = corr_value\n",
    "            p_value_matrix.loc[disease, pollutant] = p_value\n",
    "            # print(f\"{disease} and {pollutant}: {corr_value}, p={p_value}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating correlation for {disease} and {pollutant}: {e}\")\n",
    "            correlation_matrix.loc[disease, pollutant] = np.nan\n",
    "            p_value_matrix.loc[disease, pollutant] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying correlation matrix without p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the correlation matrix between airpollutants and medical conditions, since it is the easiest way to get a comprehensive view of all the correlations. \n",
    "\n",
    "Smoking amongst adults is included in the visualisation even though it's not a medical condition, we will come back to that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and transpose the correlation matrix - diseases as rows and pollutants as columns\n",
    "disease_pollutant_corr = correlation_matrix.loc[disease_columns, pollutants]\n",
    "\n",
    "# Visualize the correlation matrix as a heatmap using matplotlib\n",
    "plt.figure(figsize=(10, 14))\n",
    "\n",
    "# Convert correlation values to float\n",
    "disease_pollutant_corr = disease_pollutant_corr.astype(float)\n",
    "\n",
    "# Create the heatmap\n",
    "im = plt.imshow(disease_pollutant_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Correlation Coefficient')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Correlation between Air Pollutants and Medical Conditions', pad=50)\n",
    "plt.xticks(np.arange(len(pollutants)), pollutants, rotation=-35, ha='left')\n",
    "plt.yticks(np.arange(len(disease_columns)), disease_columns, rotation=0)  # Changed rotation to 0\n",
    "\n",
    "# Add correlation values as text annotations\n",
    "for i in range(len(disease_columns)):\n",
    "    for j in range(len(pollutants)):\n",
    "        corr_value = disease_pollutant_corr.iloc[i, j]\n",
    "        \n",
    "        # Format the correlation value\n",
    "        if pd.notna(corr_value):\n",
    "            corr_text = f'{corr_value:.2f}'\n",
    "            plt.text(j, i, corr_text, ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('corr_weighted_spearman.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating correlation matrix with p-values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the correlation matrix with the corresponding p-values to better understand the results. Correlations that are not statistically significant with 5% significanse level are grayed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 14))\n",
    "\n",
    "# Create the heatmap\n",
    "im = plt.imshow(disease_pollutant_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Correlation Coefficient')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Correlation between Air Pollutants Health Measures With p-values', pad=50)\n",
    "plt.xticks(np.arange(len(pollutants)), pollutants, rotation=-35, ha='left')\n",
    "plt.yticks(np.arange(len(disease_columns)), disease_columns, rotation=0)\n",
    "\n",
    "ax = plt.gca()  # Get the current axis\n",
    "\n",
    "# Add correlation values, p-values, and light gray overlay\n",
    "for i in range(len(disease_columns)):\n",
    "    for j in range(len(pollutants)):\n",
    "        corr_value = disease_pollutant_corr.iloc[i, j]\n",
    "        p_value = p_value_matrix.iloc[i, j]\n",
    "        \n",
    "        # Format the correlation value\n",
    "        corr_text = f'{corr_value:.2f}'\n",
    "        \n",
    "        # Format the p-value\n",
    "        if pd.notna(p_value):\n",
    "            if p_value < 0.001:\n",
    "                p_text = 'p<0.001'\n",
    "            elif p_value < 0.01:\n",
    "                p_text = 'p<0.01'\n",
    "            elif p_value < 0.05:\n",
    "                p_text = 'p<0.05'\n",
    "            else:\n",
    "                p_text = f'p={p_value:.2f}'\n",
    "        else:\n",
    "            p_text = 'p=NA'\n",
    "\n",
    "        # Add text annotations\n",
    "        plt.text(j, i - 0.15, corr_text, ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "        plt.text(j, i + 0.15, p_text, ha=\"center\", va=\"center\", color=\"black\", fontsize=6)\n",
    "\n",
    "        # Add LIGHT gray overlay if p-value >= 0.05\n",
    "        if p_value >= 0.05:\n",
    "            rect = plt.Rectangle((j - 0.5, i - 0.5), 1, 1, color='#B0B0B0')  # Light gray\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('corr_weighted_spearman_lightgray.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings from the data\n",
    "\n",
    "- From this graph we can conclude that both 'Physical health not good for >= 14 days amon adults' and 'Fair or poor self-rated health status among adults' seem to positively correlate with both pm10 and pm25 pollution levels, meaning that the overall health of individuals seems to suffer from higher amounts of pollution.\n",
    "\n",
    "- Surprisingly neither asthma or chronic obstructive pulmonary disease have any significant correlation with the pollution levels, even though both are respiratory track conditions.\n",
    "\n",
    "- Cancer among adults seems to negatively correlate with pollution levels, which seems counter intuitive. The scope of this analysis isnâ€™t deep enough to find whether the correlation is caused by a hidden variable. For this reason, we wanted to include current smoking among adults in the results, since at least in within the scope of our data it seemed to negatively correlate with air pollution levels, and as a known predictor for cancer it could partially explain the result. Here it is important to remember that correlation and causation are distinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating scatter plots of cities for the air pollutant-medical condition pairs that had statistically significant correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insignificant_mask = p_value_matrix > 0.05\n",
    "# ins_filtered_correlations = correlation_matrix.copy()\n",
    "# ins_filtered_correlations = ins_filtered_correlations.where(insignificant_mask, np.nan)\n",
    "# ins_filtered_correlations = ins_filtered_correlations.astype(float)  # Ensure numeric dtype\n",
    "\n",
    "# ins_filtered_disease_columns = pd.Series(disease_columns, index=correlation_matrix.index)  # Ensure index matches\n",
    "# invalid_rows = insignificant_mask.any(axis=1)  # Boolean mask for valid rows\n",
    "# # Apply mask after ensuring indexes match\n",
    "# ins_filtered_correlations = ins_filtered_correlations.loc[invalid_rows].reset_index(drop=True)\n",
    "# ins_filtered_disease_columns = ins_filtered_disease_columns.loc[invalid_rows].reset_index(drop=True)\n",
    "\n",
    "# print(ins_filtered_correlations)\n",
    "\n",
    "# xd_df= merged_df.copy()\n",
    "# xd_df=xd_df.drop(ins_filtered_disease_columns,axis=1)\n",
    "\n",
    "\n",
    "# disease_columns = [\n",
    "#     \"Cancer (excluding skin cancer) among adults aged >=18 years\",\n",
    "#     \"Chronic kidney disease among adults aged >=18 years\",\n",
    "#     \"Fair or poor self-rated health status among adults aged >=18 years\",\n",
    "#     \"Physical health not good for >=14 days among adults aged >=18 years\",\n",
    "# ]\n",
    "\n",
    "# pollutant_columns = [\"pm10_concentration\", \"pm25_concentration\"]  # No no2_concentration\n",
    "\n",
    "# # Set seaborn style\n",
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "# # Create scatter plots\n",
    "# for disease in disease_columns:\n",
    "#     for pollutant in pollutant_columns:\n",
    "#         plt.figure(figsize=(6, 4))\n",
    "#         sns.scatterplot(data=xd_df, x=pollutant, y=disease)\n",
    "        \n",
    "#         plt.xlabel(pollutant)\n",
    "#         plt.ylabel('age adjusted prevelance')\n",
    "#         plt.title(f\"{disease} vs {pollutant}\", fontsize=12, pad=10)  # Adjust title placement\n",
    "        \n",
    "#         plt.tight_layout()  # Ensures labels don't get cut off\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating p-values for the correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole_population = np.sum(array[:,numberOfDiseases-1])\n",
    "# print(whole_population)\n",
    "# wheighting_array = array[:,numberOfDiseases]/whole_population\n",
    "# wheighting_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This is a test correlation for one disease \n",
    "\n",
    "# X1=array[:,14]*wheighting_array\n",
    "# Y=array[:,23]\n",
    "# print(array[:,14])\n",
    "# print(X1)\n",
    "\n",
    "# x = X1.astype(float)\n",
    "# y = Y.astype(float)\n",
    "\n",
    "# mask = ~np.isnan(y)& ~np.isnan(x)\n",
    "\n",
    "# Y_filtered=y[mask]\n",
    "# X1_filtered=x[mask]\n",
    "\n",
    "# res = stats.pearsonr(X1_filtered, Y_filtered)\n",
    "\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find out whether the data is normally distributed so that we know how to compute the correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find this result extremely suspicious hence we use different test satatistics for checking normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find that the data is indeed normally distributed and as KS test is very prone to overinterpretation of outlying values and not suited for large datasets, we trust Shapiro-Wilk test and decide to go for pearson correlation test without permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = array.shape[1]\n",
    "\n",
    "air_pollution_parameter = [n-3,n-2,n-1]\n",
    "disease_range_start = 1\n",
    "disease_range_end = numberOfDiseases\n",
    "\n",
    "#permutation_model=stats.PermutationMethod(n_resamples=99999, batch=None, random_state=None, rng=None)\n",
    "\n",
    "# Ensure Corr array matches the range\n",
    "Corr_p_values = np.zeros((disease_range_end - disease_range_start + 1,len(air_pollution_parameter)))\n",
    "Corr_coef = np.zeros((disease_range_end - disease_range_start + 1,len(air_pollution_parameter)))\n",
    "\n",
    "#This computes p-values of pearson correlation test statistic between diseases and different air pollutants and puts it into a 21x3 array\n",
    "for j in range(0,len(air_pollution_parameter)):\n",
    "    y = array[:, air_pollution_parameter[j]]\n",
    "\n",
    "    for i in range(disease_range_start, disease_range_end + 1):\n",
    "        try:\n",
    "            # Extract disease column\n",
    "            a = array[:, i]#*wheighting_array\n",
    "            \n",
    "            # Convert to float as datasets are super messy\n",
    "            a_float = a.astype(float)\n",
    "            y_float = y.astype(float)\n",
    "            \n",
    "            # Create mask for non-NaN values in BOTH columns\n",
    "            mask =  ~np.isnan(y_float) & ~np.isnan(a_float) \n",
    "            \n",
    "            # Filter both arrays\n",
    "            y_filtered = y_float[mask]\n",
    "            a_filtered = a_float[mask]\n",
    "            \n",
    "            # Compute Pearson correlation\n",
    "            res = stats.pearsonr(a_filtered, y_filtered)\n",
    "            \n",
    "            Corr_p_values[i - disease_range_start,j] = res.pvalue\n",
    "            Corr_coef[i - disease_range_start,j]=res.correlation\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column {i}: {e}\")\n",
    "\n",
    "print(Corr_coef)\n",
    "Corr_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a scatter plot of diseases and thier p-values\n",
    "\n",
    "features = np.arange(1, len(Corr_p_values) + 1)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(features, Corr_p_values[:,0], color='blue', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_p_values[:,1], color='green', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_p_values[:,2], color='red', edgecolors='black', alpha=0.7)\n",
    "# Red line at y=0.05\n",
    "plt.axhline(y=0.05, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.xlim(1, len(Corr_p_values))\n",
    "\n",
    "plt.xticks(features)\n",
    "\n",
    "plt.xlabel('Diseases')\n",
    "plt.ylabel('correlation p-value')\n",
    "plt.title('Correlation Scatter Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify lengths\n",
    "print(\"Length of Corr:\", len(Corr_p_values))\n",
    "print(\"Length of features:\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This filters the Corr matrix so that only statistically significant values (p<=0.05) remain\n",
    "print(Corr_p_values.shape)\n",
    "\n",
    "Corr_significat_p=Corr_p_values.copy() #if I dont use copy(), Corr also changes when I change Corr_significant\n",
    "Corr_significat_p[Corr_significat_p > 0.05] = np.nan\n",
    "print(Corr_significat_p.shape)\n",
    "print(Corr_significat_p)\n",
    "\n",
    "#This creates a scatter plot with statistically significant values\n",
    "\n",
    "features = np.arange(1, len(Corr_significat_p) + 1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(features, Corr_significat_p[:,0], color='blue', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_significat_p[:,1], color='green', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_significat_p[:,2], color='red', edgecolors='black', alpha=0.7)\n",
    "# Red line at y=0.05\n",
    "plt.axhline(y=0.05, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.xlim(1, len(Corr_p_values))\n",
    "\n",
    "plt.xticks(features)\n",
    "\n",
    "plt.xlabel('Diseases')\n",
    "plt.ylabel('correlation p-value')\n",
    "plt.title('Correlation Scatter Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify lengths\n",
    "print(\"Length of Corr_significant:\", len(Corr_significat_p))\n",
    "print(\"Length of features:\", len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as I want to present the data as a table I switch to dataframes as it makes it easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_p_value_df= pd.DataFrame(Corr_p_values, index=disease_columns, columns=pollutants)\n",
    "\n",
    "significant_correlation_p_value_df = correlation_p_value_df.where(correlation_p_value_df <= 0.05, np.nan)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=correlation_p_value_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=correlation_p_value_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=significant_correlation_p_value_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=significant_correlation_p_value_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to filter the correlation coefficient data so that it leaves only the statistically signifficant coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_Corr_coef = np.where(Corr_p_values < 0.05, Corr_coef, np.nan)\n",
    "sign_Corr_coef\n",
    "\n",
    "correlation_coef_df= pd.DataFrame(Corr_coef, index=disease_columns, columns=pollutants)\n",
    "sign_correlation_coef_df= pd.DataFrame(sign_Corr_coef, index=disease_columns, columns=pollutants)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=correlation_coef_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=correlation_coef_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=sign_correlation_coef_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=sign_correlation_coef_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
