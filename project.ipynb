{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PLACES_Local_Data_for_Better_Health_County_Data_2022_release_20250320.csv\")\n",
    "\n",
    "df = df[df[\"Data_Value_Type\"].str.contains(\"Crude\", na=False)]\n",
    "df = df[df['TotalPopulation'] > 100000]\n",
    "\n",
    "# Rename the city column\n",
    "df = df.rename(columns={\"LocationName\": \"city\"})\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(['StateAbbr', 'StateDesc', 'DataSource', 'Category', 'LocationID', 'CategoryID', 'DataValueTypeID', 'Geolocation', 'Low_Confidence_Limit', 'High_Confidence_Limit', 'Data_Value_Footnote_Symbol', 'MeasureId', 'Data_Value_Footnote' ], axis=1)\n",
    "# First, let's identify the unique cities and measures\n",
    "unique_cities = df['city'].unique()\n",
    "unique_measures = df['Short_Question_Text'].unique()\n",
    "\n",
    "print(f\"Number of unique cities: {len(unique_cities)}\")\n",
    "print(f\"Number of unique measures: {len(unique_measures)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a pivot table with cities as index and measures as columns\n",
    "new_df = df.pivot_table(\n",
    "    index='city',\n",
    "    columns='Measure',\n",
    "    values='Data_Value',\n",
    "    aggfunc='mean'  # Use mean if there are duplicate entries\n",
    ")\n",
    "\n",
    "population_df = df.groupby('city')['TotalPopulation'].first().reset_index()\n",
    "\n",
    "# Reset the index to make 'city' a column again\n",
    "new_df = new_df.reset_index()\n",
    "\n",
    "new_df = new_df.drop(['Current lack of health insurance among adults aged 18-64 years',\n",
    "'Cervical cancer screening among adult women aged 21-65 years',\n",
    "'Visits to dentist or dental clinic among adults aged >=18 years',\n",
    " 'Visits to doctor for routine checkup within the past year among adults aged >=18 years',\n",
    " 'Mammography use among women aged 50-74 years',\n",
    " 'Cholesterol screening among adults aged >=18 years',\n",
    " 'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',\n",
    " 'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',\n",
    " 'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',\n",
    "], axis=1)\n",
    "\n",
    "# All diseases\n",
    "# list(df['Measure'].unique())\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airdf = pd.read_excel(\"who_ambient_air_quality_database_version_2024_(v6.1).xlsx\", sheet_name=\"Update 2024 (V6.1)\")\n",
    "\n",
    "\n",
    "# Filter by US\n",
    "airdf = airdf[airdf[\"country_name\"] == \"United States of America\"]\n",
    "\n",
    "# Drop unnnecessary columns\n",
    "airdf = airdf.drop(['country_name', 'version','reference','web_link', 'population_source','who_ms', 'type_of_stations', 'population', 'latitude', 'longitude', 'iso3', 'who_region', 'pm25_tempcov', 'pm10_tempcov','no2_tempcov'], axis=1)\n",
    "\n",
    "# Remove state from city names\n",
    "airdf['city'] = airdf['city'].str.split(' ').str[0]\n",
    "airdf['city'] = airdf['city'].str.split('-').str[0]\n",
    "# Leave only year 2020\n",
    "airdf = airdf[airdf['year'] == 2019.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_best_match_city(city, citylist, threshold=90):\n",
    "#     best_match, score = process.extractOne(city, citylist)\n",
    "#     if score >= threshold:\n",
    "#         return best_match\n",
    "#     else: city\n",
    "\n",
    "# airdf['city'] = airdf['city'].apply(lambda x: find_best_match_city(x, unique_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(new_df, airdf, on='city', how='inner')\n",
    "merged_df = pd.merge(merged_df, population_df, on='city', how='left')\n",
    "merged_df = merged_df.drop_duplicates(subset='city', keep='first')\n",
    "\n",
    "# Optional: print some information about the merged DataFrame\n",
    "print(merged_df.shape)\n",
    "print(\"Columns with non-null PM10 concentrations:\", merged_df['pm10_concentration'].notna().sum())\n",
    "print(\"Columns with non-null PM2.5 concentrations:\", merged_df['pm25_concentration'].notna().sum())\n",
    "print(\"Columns with non-null NO2 concentrations:\", merged_df['no2_concentration'].notna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all column names to identify the correct ones\n",
    "print(\"Merged dataframe columns:\", merged_df.columns.tolist())\n",
    "\n",
    "# Update these with the ACTUAL column names from your print output\n",
    "pollutants = ['pm10_concentration', 'pm25_concentration', 'no2_concentration']  # Use the correct names\n",
    "\n",
    "# Get disease columns\n",
    "disease_columns = [col for col in new_df.columns if col != 'city']\n",
    "\n",
    "# Create a correlation matrix between pollutants and diseases\n",
    "correlation_matrix = merged_df[pollutants + disease_columns].corr()\n",
    "\n",
    "# Extract and transpose the correlation matrix - diseases as rows and pollutants as columns\n",
    "disease_pollutant_corr = correlation_matrix.loc[disease_columns, pollutants]\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation between health measures and air pollutants:\")\n",
    "disease_pollutant_corr\n",
    "\n",
    "# Visualize the correlation matrix as a heatmap using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 14))  # Adjusted for more rows than columns\n",
    "im = plt.imshow(disease_pollutant_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Correlation Coefficient')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Correlation between Health Measures and Air Pollutants')\n",
    "plt.xticks(np.arange(len(pollutants)), pollutants, rotation=45)\n",
    "plt.yticks(np.arange(len(disease_columns)), disease_columns)\n",
    "\n",
    "# Add correlation values as text annotations\n",
    "for i in range(len(disease_columns)):\n",
    "    for j in range(len(pollutants)):\n",
    "        text = plt.text(j, i, f'{disease_pollutant_corr.iloc[i, j]:.2f}', ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('corr.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This puts merged df into an array to use Scipy\n",
    "\n",
    "array = merged_df.to_numpy()\n",
    "\n",
    "print(array.shape)\n",
    "\n",
    "array[:,6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_population = np.sum(array[:,26])\n",
    "print(whole_population)\n",
    "wheighting_array = array[:,26]/whole_population\n",
    "wheighting_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a test correlation for one disease \n",
    "\n",
    "X1=array[:,14]*wheighting_array\n",
    "Y=array[:,23]\n",
    "print(array[:,14])\n",
    "print(X1)\n",
    "\n",
    "x = X1.astype(float)\n",
    "y = Y.astype(float)\n",
    "\n",
    "mask = ~np.isnan(y)& ~np.isnan(x)\n",
    "\n",
    "Y_filtered=y[mask]\n",
    "X1_filtered=x[mask]\n",
    "\n",
    "res = stats.pearsonr(X1_filtered, Y_filtered)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find out whether the data is normally distributed so that we know how to compute the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_data = np.array(array[:,6], dtype=float)  # Convert to float\n",
    "\n",
    "# Perform KS test against normal distribution\n",
    "ks_statistic, p_value = stats.kstest(column_data, 'norm')\n",
    "# Normalise for the KS test\n",
    "column_data = (column_data - np.mean(column_data)) / np.std(column_data)\n",
    "\n",
    "print(f\"KS Statistic: {ks_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: Data is NOT normally distributed.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: Data is likely normally distributed.\")\n",
    "\n",
    "\n",
    "#Here we note that the result is extremely suspicious, therfore we perform different tests(probably more suited for large datasets and normality testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find this result extremely suspicious hence we use different test satatistics for checking normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapiro-Wilk normality test\n",
    "stat, p_value = shapiro(column_data)\n",
    "print(f\"Shapiro-Wilk p-value: {p_value}\")\n",
    "\n",
    "# D'Agostino and Pearson's test\n",
    "from scipy.stats import normaltest\n",
    "stat, p_value = normaltest(column_data)\n",
    "print(f\"D'Agostino-Pearson p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find that the data is indeed normally distributed and as KS test is very prone to overinterpretation of outlying values and not suited for large datasets, we trust Shapiro-Wilk test and decide to go for pearson correlation test without permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_pollution_parameter = [23,24,25]\n",
    "disease_range_start = 1\n",
    "disease_range_end = 21\n",
    "\n",
    "#permutation_model=stats.PermutationMethod(n_resamples=99999, batch=None, random_state=None, rng=None)\n",
    "\n",
    "# Ensure Corr array matches the range\n",
    "Corr_p_values = np.zeros((disease_range_end - disease_range_start + 1,len(air_pollution_parameter)))\n",
    "Corr_coef = np.zeros((disease_range_end - disease_range_start + 1,len(air_pollution_parameter)))\n",
    "\n",
    "#This computes p-values of pearson correlation test statistic between diseases and different air pollutants and puts it into a 21x3 array\n",
    "for j in range(0,len(air_pollution_parameter)):\n",
    "    y = array[:, air_pollution_parameter[j]]\n",
    "\n",
    "    for i in range(disease_range_start, disease_range_end + 1):\n",
    "        try:\n",
    "            # Extract disease column\n",
    "            a = array[:, i]#*wheighting_array\n",
    "            \n",
    "            # Convert to float as datasets are super messy\n",
    "            a_float = a.astype(float)\n",
    "            y_float = y.astype(float)\n",
    "            \n",
    "            # Create mask for non-NaN values in BOTH columns\n",
    "            mask =  ~np.isnan(y_float) & ~np.isnan(a_float) \n",
    "            \n",
    "            # Filter both arrays\n",
    "            y_filtered = y_float[mask]\n",
    "            a_filtered = a_float[mask]\n",
    "            \n",
    "            # Compute Pearson correlation\n",
    "            res = stats.pearsonr(a_filtered, y_filtered)\n",
    "            \n",
    "            Corr_p_values[i - disease_range_start,j] = res.pvalue\n",
    "            Corr_coef[i - disease_range_start,j]=res.correlation\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column {i}: {e}\")\n",
    "\n",
    "print(Corr_coef)\n",
    "Corr_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a scatter plot of diseases and thier p-values\n",
    "\n",
    "features = np.arange(1, len(Corr_p_values) + 1)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(features, Corr_p_values[:,0], color='blue', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_p_values[:,1], color='green', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_p_values[:,2], color='red', edgecolors='black', alpha=0.7)\n",
    "# Red line at y=0.05\n",
    "plt.axhline(y=0.05, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.xlim(1, len(Corr_p_values))\n",
    "\n",
    "plt.xticks(features)\n",
    "\n",
    "plt.xlabel('Diseases')\n",
    "plt.ylabel('correlation p-value')\n",
    "plt.title('Correlation Scatter Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify lengths\n",
    "print(\"Length of Corr:\", len(Corr_p_values))\n",
    "print(\"Length of features:\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This filters the Corr matrix so that only statistically significant values (p<=0.05) remain\n",
    "print(Corr_p_values.shape)\n",
    "\n",
    "Corr_significat_p=Corr_p_values.copy() #if I dont use copy(), Corr also changes when I change Corr_significant\n",
    "Corr_significat_p[Corr_significat_p > 0.05] = np.nan\n",
    "print(Corr_significat_p.shape)\n",
    "print(Corr_significat_p)\n",
    "\n",
    "#This creates a scatter plot with statistically significant values\n",
    "\n",
    "features = np.arange(1, len(Corr_significat_p) + 1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(features, Corr_significat_p[:,0], color='blue', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_significat_p[:,1], color='green', edgecolors='black', alpha=0.7)\n",
    "plt.scatter(features, Corr_significat_p[:,2], color='red', edgecolors='black', alpha=0.7)\n",
    "# Red line at y=0.05\n",
    "plt.axhline(y=0.05, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.xlim(1, len(Corr_p_values))\n",
    "\n",
    "plt.xticks(features)\n",
    "\n",
    "plt.xlabel('Diseases')\n",
    "plt.ylabel('correlation p-value')\n",
    "plt.title('Correlation Scatter Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify lengths\n",
    "print(\"Length of Corr_significant:\", len(Corr_significat_p))\n",
    "print(\"Length of features:\", len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as I want to present the data as a table I switch to dataframes as it makes it easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_p_value_df= pd.DataFrame(Corr_p_values, index=disease_columns, columns=pollutants)\n",
    "\n",
    "significant_correlation_p_value_df = correlation_p_value_df.where(correlation_p_value_df <= 0.05, np.nan)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=correlation_p_value_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=correlation_p_value_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=significant_correlation_p_value_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=significant_correlation_p_value_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to filter the correlation coefficient data so that it leaves only the statistically signifficant coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_Corr_coef = np.where(Corr_p_values < 0.05, Corr_coef, np.nan)\n",
    "sign_Corr_coef\n",
    "\n",
    "correlation_coef_df= pd.DataFrame(Corr_coef, index=disease_columns, columns=pollutants)\n",
    "sign_correlation_coef_df= pd.DataFrame(sign_Corr_coef, index=disease_columns, columns=pollutants)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=correlation_coef_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=correlation_coef_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.table(\n",
    "    cellText=sign_correlation_coef_df.values, \n",
    "    colLabels=[\"Disease/Air Pollution\"] + pollutants, \n",
    "    rowLabels=sign_correlation_coef_df.index, \n",
    "    cellLoc='center', \n",
    "    loc='center'\n",
    ")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
